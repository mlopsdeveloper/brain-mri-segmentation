{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea5c88dc-d1dd-4f74-bd24-75335a5ac275",
   "metadata": {},
   "source": [
    "This notebook reuses code from https://www.kaggle.com/code/agggshambhavi/pytorch-brain-mri-binary-segmentation\n",
    "\n",
    "It shows how to build the DataLoaders knowing the directory where the data is available, show some data samples and examples of data augmentation. It also trains the model and use it to make some predictions. Most of its content is reused in data.py and train.py."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1e28e8-c05b-4b8d-b2bd-dd7b371a69ed",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459f3242-ffea-467a-8900-13cdd3b2fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing required module\n",
    "import subprocess\n",
    " \n",
    "# Using system() method to\n",
    "# execute shell commands\n",
    "p1 = subprocess.Popen('python -m venv ../.venv/brain-mri', shell=True)\n",
    "p1.wait()\n",
    "p2 = subprocess.Popen('source ../.venv/brain-mri/bin/activate', shell=True)\n",
    "p2.wait()\n",
    "p3 = subprocess.Popen('/bin/sh startup-hook.sh', shell=True)\n",
    "p3.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f088fc8-576b-4ff5-9f8a-025b3f6cfe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31974b86-319d-4d81-9512-fab41b394248",
   "metadata": {},
   "source": [
    "# Defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2be069-71ef-4388-8547-8b39d1b65616",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/lgg-mri-segmentation/kaggle_3m/\" # change the path accordingly\n",
    "\n",
    "train_fraction = 0.8\n",
    "validation_fraction = 0.2\n",
    "batch_size = 16\n",
    "\n",
    "input_dim = 256\n",
    "input_ch = 3\n",
    "output_dim = 256\n",
    "output_ch = 1\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 25\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75986e8-01bd-4945-8627-435a175c667e",
   "metadata": {},
   "source": [
    "# Reading images path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f41a4-d281-46fe-8bbe-30589fbd51fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs, images, masks = [], [], []\n",
    "\n",
    "for root, folders, files in  os.walk(data_dir):\n",
    "    for file in files:\n",
    "        if 'mask' in file:\n",
    "            dirs.append(root.replace(data_dir, ''))\n",
    "            masks.append(file)\n",
    "            images.append(file.replace(\"_mask\", \"\"))\n",
    "\n",
    "PathDF = pd.DataFrame({'directory': dirs,\n",
    "                      'images': images,\n",
    "                      'masks': masks})\n",
    "PathDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e0dae3-1ae3-4f2f-b20b-faf8c7c88fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in PathDF.index:\n",
    "    PathDF.loc[i, \"diagnosis\"] = 1 if np.max(cv2.imread(os.path.join(data_dir, PathDF.loc[i, 'directory'], PathDF.loc[i,\"masks\"]))) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d657b1-b754-4c61-b1ff-06768e749b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "PathDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb9121-15a7-4ca3-9253-18755087dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(PathDF, random_state=seed,\n",
    "                                 test_size = validation_fraction)\n",
    "\n",
    "print('Train:', train_df.shape[0])\n",
    "print('Valid:', valid_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29a825-12ca-4b2b-a25b-5c0d67edb9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b768144-57c7-4757-ac93-882c4cb2883c",
   "metadata": {},
   "source": [
    "# Creating Torch Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71030a00-0a97-473d-80e7-a3b023e6c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRI_Dataset(Dataset):\n",
    "    def __init__(self, path_df, transform=None):\n",
    "        self.path_df = path_df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.path_df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        base_path = os.path.join(data_dir, self.path_df.iloc[idx]['directory'])\n",
    "        img_path = os.path.join(base_path, self.path_df.iloc[idx]['images'])\n",
    "        mask_path = os.path.join(base_path, self.path_df.iloc[idx]['masks'])\n",
    "        \n",
    "        image = Image.open(img_path)\n",
    "        mask = Image.open(mask_path)\n",
    "        \n",
    "        sample = (image, mask)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b21d32-47b6-423e-973e-1aa09153c5c7",
   "metadata": {},
   "source": [
    "Preprocessing function to apply to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe76ffc1-e3f0-437a-9cff-199d0dfc52e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedToTensor():\n",
    "    def __call__(self, sample):\n",
    "        img, mask = sample\n",
    "        img = np.array(img)\n",
    "        mask = np.expand_dims(mask, -1)\n",
    "        img = np.moveaxis(img, -1, 0)\n",
    "        mask = np.moveaxis(mask, -1, 0)\n",
    "        img, mask = torch.FloatTensor(img), torch.FloatTensor(mask)\n",
    "        img = img/255\n",
    "        mask = mask/255\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60fde75-8570-4ea0-baf4-a60e6bca44bd",
   "metadata": {},
   "source": [
    "Data transformation techniques for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7096bc0-6d7e-452a-bb8d-392ec8723272",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedRandomHorizontalFlip():\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        img, mask = sample\n",
    "        if np.random.random() < self.p:\n",
    "            img, mask = TF.hflip(img), TF.hflip(mask)\n",
    "            \n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35c16fd-ebf4-4d96-b1ab-c7c9eb5435dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedRandomAffine():\n",
    "    \n",
    "    def __init__(self, degrees= None, translate=None, scale_ranges=None,\n",
    "                shears=None):\n",
    "        self.params = {\n",
    "            'degree': degrees,\n",
    "            'translate': translate,\n",
    "            'scale_ranges':scale_ranges,\n",
    "            'shears':shears\n",
    "        }\n",
    "    def __call__(self, sample):\n",
    "        img, mask = sample\n",
    "        w, h = img.size\n",
    "        \n",
    "        angle, translations, scale, shear = transforms.RandomAffine.get_params(\n",
    "            self.params['degree'], self.params['translate'],\n",
    "            self.params['scale_ranges'], self.params['shears'],\n",
    "            (w,h)\n",
    "        )\n",
    "        \n",
    "        img = TF.affine(img, angle, translations, scale, shear)\n",
    "        mask = TF.affine(mask, angle, translations, scale, shear)\n",
    "        \n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a03562-be58-4f72-904c-3486fc18ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(sample, title=None):\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(sample[0])\n",
    "    ax[1].imshow(sample[1], cmap=\"gray\")\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325f8727-8afe-42dd-a105-11b737b9575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(data_dir, train_df.iloc[0]['directory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad448b-60c9-4877-b149-bd74758c6758",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MRI_Dataset(train_df)\n",
    "sample = dataset[0]\n",
    "transform = PairedRandomHorizontalFlip(p=1)\n",
    "show_sample(sample, title='Original')\n",
    "show_sample(transform(sample), title=\"Horizontal Flip\")\n",
    "transform = PairedRandomAffine(\n",
    "    degrees = (15,15),\n",
    "    scale_ranges = (1.2, 1.2),\n",
    "    translate = (0.1,0.1)\n",
    ")\n",
    "show_sample(transform(sample), 'Affine Transformed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6216762e-7e3d-479e-922c-1e50b1d12861",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose(\n",
    "    [PairedRandomHorizontalFlip(),\n",
    "    PairedRandomAffine(\n",
    "        degrees=(-15, 15),\n",
    "        translate=(0.1, 0.1),\n",
    "        scale_ranges=(0.8, 1.2)\n",
    "    ),\n",
    "    PairedToTensor()\n",
    "    ])\n",
    "\n",
    "train_data = MRI_Dataset(train_df, transform=train_transforms)\n",
    "valid_data = MRI_Dataset(valid_df, transform=PairedToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size,\n",
    "                         shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce0a93a-0a74-4d82-9c4c-09dd182135ce",
   "metadata": {},
   "source": [
    "# Loading pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba60b8-506f-4f59-aaa4-9baf2f33c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "    in_channels=3, out_channels=1, init_features=32, pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6efd14b-76fe-4371-b2e2-873a7dcbfae7",
   "metadata": {},
   "source": [
    "# Use GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031a318e-8cad-4551-86b6-b61ea7854ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feb360b-1c51-443c-91cd-16881c0a7c27",
   "metadata": {},
   "source": [
    "# Defining utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48078cf4-7461-42bf-8b3c-2f7f5c63d086",
   "metadata": {},
   "source": [
    "Custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867daa95-edc2-420d-8033-68153f6e2a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from https://www.kaggle.com/code/lqdisme/brain-mri-segmentation-unet-pytorch/notebook\n",
    "def dice_coef_metric(pred, label):\n",
    "    intersection = 2.0 * (pred * label).sum()\n",
    "    union = pred.sum() + label.sum()\n",
    "    if pred.sum() == 0 and label.sum() == 0:\n",
    "        return 1\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae36c89-c83b-4219-a610-f7ff900bfbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(pred, label):\n",
    "    intersection = (pred * label).sum()\n",
    "    union = pred.sum() + label.sum() - intersection\n",
    "    if pred.sum() == 0 and label.sum() == 0:\n",
    "        return 1\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36851d1e-e9cc-446b-9b5f-f60914f14118",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2a4ec6-a06d-468e-a524-31794e680335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, criterion, train_loader):\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    final_dice_coef = 0 \n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        imgs, masks = data\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # forward\n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, masks)\n",
    "        running_loss += loss.item() * imgs.shape[0]\n",
    "        \n",
    "        out_cut = np.copy(out.detach().cpu().numpy())\n",
    "        out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n",
    "        out_cut[np.nonzero(out_cut >= 0.5)] = 1.0\n",
    "            \n",
    "        train_dice = dice_coef_metric(out_cut, masks.data.cpu().numpy())\n",
    "        final_dice_coef += train_dice \n",
    "        \n",
    "        # optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    running_loss /= len(train_loader.sampler)\n",
    "    return {'dice coef':final_dice_coef/len(train_loader), \n",
    "                'loss':running_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9428e39-1d7e-4f33-a239-9e2ad723d555",
   "metadata": {},
   "source": [
    "Validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2093c975-10ee-425f-b03d-1034814db72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(model, criterion, eval_loader):\n",
    "    \n",
    "    running_loss = 0\n",
    "    final_dice_coef = 0 \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, data in enumerate(eval_loader, 0):\n",
    "            \n",
    "            imgs, masks = data\n",
    "            \n",
    "            imgs = imgs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            out = model(imgs)\n",
    "            loss = criterion(out, masks)\n",
    "            running_loss += loss.item() * imgs.shape[0]\n",
    "            \n",
    "            out_cut = np.copy(out.detach().cpu().numpy())\n",
    "            out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n",
    "            out_cut[np.nonzero(out_cut >= 0.5)] = 1.0\n",
    "            \n",
    "            valid_dice = dice_coef_metric(out_cut, masks.data.cpu().numpy())\n",
    "            final_dice_coef += valid_dice \n",
    "            \n",
    "    running_loss /= len(eval_loader.sampler)   \n",
    "    return {\n",
    "                'dice coef':final_dice_coef/len(eval_loader), \n",
    "                'loss':running_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9729ac-8b93-4085-9562-f4fb9b624489",
   "metadata": {},
   "source": [
    "General training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0692d2f5-b736-4142-aae3-6963495bf3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, scheduler, train_loader, valid_loader,\n",
    "          num_epochs = epochs,\n",
    "          valid_loss_min = np.inf):\n",
    "    \n",
    "    train_loss_list = []\n",
    "    train_dice_coef = []\n",
    "    val_loss_list = []\n",
    "    val_dice_coef = []\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        \n",
    "        train_metrics = train_loop(model, optimizer, criterion, \n",
    "                                   train_loader)\n",
    "        \n",
    "        val_metrics = eval_loop(model, criterion, valid_loader)\n",
    "        \n",
    "        scheduler.step(val_metrics['dice coef'])\n",
    "        \n",
    "        train_loss_list.append(train_metrics['loss']) \n",
    "        train_dice_coef.append(train_metrics['dice coef'])\n",
    "        val_loss_list.append(val_metrics['loss'])\n",
    "        val_dice_coef.append(val_metrics['dice coef'])\n",
    "        \n",
    "        print_string = f\"Epoch: {e+1}\\n\"\n",
    "        print_string += f\"Train Loss: {train_metrics['loss']:.5f}\\n\"\n",
    "        print_string += f\"Train Dice Coef: {train_metrics['dice coef']:.5f}\\n\"\n",
    "        print_string += f\"Valid Loss: {val_metrics['loss']:.5f}\\n\"\n",
    "        print_string += f\"Valid Dice Coef: {val_metrics['dice coef']:.5f}\\n\"\n",
    "        print(print_string)\n",
    "        \n",
    "        # save model\n",
    "        #if val_metrics[\"loss\"] <= valid_loss_min:\n",
    "        #    torch.save(model.state_dict(), \"UNET.pt\")\n",
    "        #    valid_loss_min = val_metrics[\"loss\"]\n",
    "        \n",
    "    return [train_loss_list,\n",
    "    train_dice_coef,\n",
    "    val_loss_list,\n",
    "    val_dice_coef]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d69c4b-b557-433e-8088-ca25181b1443",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c7cfe-cc1c-4e12-aac2-a8e2f0127408",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "criterion = nn.BCELoss(reduction='mean')\n",
    "# only one epoch, just to show that we could train the model in the notebook\n",
    "train_loss_list, train_dice_coef,val_loss_list,val_dice_coef = train(\n",
    "    model, optimizer, criterion, scheduler, train_loader, valid_loader, num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821655c7-e0ba-4d26-8a2e-789ec26dbe63",
   "metadata": {},
   "source": [
    "# Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1956b7c5-3164-4f22-acfc-0323774ed8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred(model, sample_idx):\n",
    "    sample_img, sample_mask = valid_data[sample_idx]\n",
    "    sample_img_tensor = torch.FloatTensor(np.expand_dims(sample_img, 0))\n",
    "    sample_img_tensor = sample_img_tensor.to(device)\n",
    "\n",
    "    sample_img = torch.Tensor(sample_img).permute(1,2,0)\n",
    "    \n",
    "    model.eval()\n",
    "    sample_pred = model(sample_img_tensor)\n",
    "    sample_pred = sample_pred.cpu().detach().numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(20,10))\n",
    "    ax[0].title.set_text(\"Original image\")\n",
    "    ax[0].imshow(sample_img)\n",
    "    \n",
    "    ax[1].title.set_text(\"Ground truth\")\n",
    "    ax[1].imshow(sample_mask[0], cmap=\"gray\")\n",
    "    \n",
    "    ax[2].title.set_text(\"Prediction\")\n",
    "    ax[2].imshow(sample_pred[0][0], cmap=\"gray\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c3d39d-d1e7-4d0d-800f-dc8760bacbb8",
   "metadata": {},
   "source": [
    "It is supposed to be bad if we train for a single epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb088b-19f5-48e9-ab07-eea47c914a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred(model, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
